{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephen-crone/monkeypox/blob/main/model-retesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdOh0eUFsWSh"
      },
      "source": [
        "# **<br>Detecting Misinformation and Superspreaders in Social Media:<br>Designing a System for the Next Pandemic**\n",
        "#Phase 5:  Evaluating model resilience to temporal shift\n",
        "\n",
        "Stephen Crone\n",
        "<br>\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGGR0Yzvao-"
      },
      "source": [
        "---\n",
        "## **1. Setting up programming environment**\n",
        "Our first step will be to install and import the libraries we need.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IanCffMyhksH"
      },
      "source": [
        "###**1.1. Installing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yFOxOtW1sMiB"
      },
      "outputs": [],
      "source": [
        "# For access to transformer models.\n",
        "!pip install transformers\n",
        "# For operations on HuggingFace dataset objects.\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmaJvm6fhmwq"
      },
      "source": [
        "###**1.2. Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubMBBRjv0IPN"
      },
      "outputs": [],
      "source": [
        "# For general dataset manipulation.\n",
        "from datasets import ClassLabel, Dataset, DatasetDict, Features, Value\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "# For dataset preprocessing.\n",
        "from transformers import AutoTokenizer, DefaultDataCollator\n",
        "# For model predictions / evaluation.\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "# For visualisation.\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "# Other imports.\n",
        "import random as python_random\n",
        "import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "RJT78Mh3arZw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz198uQLnVqJ"
      },
      "source": [
        "---\n",
        "## **2. Importing the dataset**\n",
        "In this section, we load the monkeypox misinformation follow-up dataset and create two versions to select from: the first a full version (based on the 'misinformation' vs 'other' class split); the second a smaller subset (based on the 'misinformation' vs 'good information' class split).\n",
        "\n",
        "Please note: user must upload a Kaggle API token to session storage in order to successfully download the dataset. Failure to do so will generate an error.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sraK1NtCwt8V"
      },
      "outputs": [],
      "source": [
        "# Preparing Kaggle and Kaggle API token.\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# Downloading the dataset from Kaggle.\n",
        "! kaggle datasets download stephencrone/monkeypox\n",
        "# Unzipping the dataset.\n",
        "! unzip monkeypox\n",
        "# Assigning dataset to Pandas DataFrame.\n",
        "bigDF = pd.read_csv('/content/monkeypox-followup.csv')\n",
        "# Reconfiguring datetime features.\n",
        "bigDF['created_at'] = pd.to_datetime(bigDF['created_at'])\n",
        "bigDF['user created at'] = pd.to_datetime(bigDF['user created at'])\n",
        "# Creating a second, smaller DataFrame where we replace 'other' (i.e. non-misinformation)\n",
        "# class with 'good' (i.e. reliable information) class.\n",
        "littleDF = bigDF.copy()\n",
        "littleDF = littleDF.drop(littleDF[littleDF.ternary_class == 9].index)\n",
        "littleDF = littleDF.drop(['binary_class'],axis=1)\n",
        "littleDF = littleDF.rename(columns={\"ternary_class\": \"class\"})\n",
        "# Removing redundant class label column from bigDF.\n",
        "bigDF = bigDF.drop(['ternary_class'],axis=1)\n",
        "bigDF = bigDF.rename(columns={\"binary_class\": \"class\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by1ZPVt5l0q2"
      },
      "source": [
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx2JNqjQyR26"
      },
      "source": [
        "---\n",
        "## **3. Exploratory data analysis (EDA)**\n",
        "There is no need to subject this follow-up dataset to the same level of EDA as we subjected the larger dataset (on which the final model was trained). However, as a minimum, we will quickly review the features of the dataset and the distribution of the class labels. In addition, we will look at at the distribution of the newly-added 'beto_flag' feature (a Boolean feature that denotes whether or not the tweet references the Texan Democratic politician, Beto O'Rourke), which has been added due to [false reports](https://www.reuters.com/article/factcheck-beto-bacterial-infection-idUSL1N3051YN) of his infection with monkeypox that were conspicuously prominent during annotation of the data.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh_SO8To0w-U"
      },
      "outputs": [],
      "source": [
        "# Review dataset head.\n",
        "bigDF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uWqsSHN50W7"
      },
      "outputs": [],
      "source": [
        "# Review dataset info.\n",
        "bigDF.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB-6W8edEGbk"
      },
      "outputs": [],
      "source": [
        "# Show class distribution for larger version of dataset.\n",
        "bigDF['class'].value_counts(normalize=True).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O-IegwG0S1U"
      },
      "outputs": [],
      "source": [
        "# Show class distribution for smaller version of dataset.\n",
        "littleDF['class'].value_counts(normalize=True).sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show distribution of beto_flag feature for larger version of dataset.\n",
        "bigDF['beto_flag'].value_counts(normalize=True).sort_index()"
      ],
      "metadata": {
        "id": "6ywug42CqvRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show distribution of beto_flag feature for smaller version of dataset.\n",
        "littleDF['beto_flag'].value_counts(normalize=True).sort_index()"
      ],
      "metadata": {
        "id": "THKWy7eks8hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "KEgFzZrIaw4j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhjs5-KbHgOL"
      },
      "source": [
        "---\n",
        "## **4. Preprocessing the dataset**\n",
        "For Phase 5, we will preprocess the dataset in exactly the same way that we did the winning model from Phase 2. This means: (i) selecting the larger version of the dataset (with 'misinformation' vs. 'non-misinformation' class labels); and (ii) allowing the model to see the same combination of features that the winning model saw during training (i.e. the tweet text plus the user verification column). With those steps concluded, the dataset must be tokenized and converted into a format amenable to processing by Tensorflow.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide whether to test 'Beto theory': that decline in classifier performance can be accounted for in part\n",
        "# by emergence of false claims regarding Beto O'Rourke's purported monkeypox infection.\n",
        "exploreBetoFactor = False\n",
        "if exploreBetoFactor == True:\n",
        "  bigDF = bigDF.drop(bigDF[bigDF.beto_flag == False].index)\n",
        "  littleDF = littleDF.drop(littleDF[littleDF.beto_flag == False].index)"
      ],
      "metadata": {
        "id": "t-VLOl9grL-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGEhm2nt_ldm"
      },
      "outputs": [],
      "source": [
        "# Selecting larger dataset for consistency with Phase 2 winning model.\n",
        "datasetChoice = bigDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting same dataset features as Phase 2 winning model.\n",
        "chosenFeatures = ['text', 'user is verified', 'class']\n",
        "featuresToConcatenate = chosenFeatures.copy()\n",
        "featuresToConcatenate.remove('class')"
      ],
      "metadata": {
        "id": "NAOjFhT6Zy7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating the desired features in a copy of the dataframe.\n",
        "slimmedDownDF = datasetChoice[chosenFeatures].copy()\n",
        "for i in featuresToConcatenate:\n",
        "  slimmedDownDF[i] = slimmedDownDF[i].name + \": \" + slimmedDownDF[i].astype(str)\n",
        "slimmedDownDF['combined'] = slimmedDownDF[featuresToConcatenate].apply(lambda row: ' [SEP] '.join(row.values.astype(str)), axis=1)\n",
        "finalDF = slimmedDownDF[['combined','class']].copy()\n",
        "# Assigning correct class labels.\n",
        "if datasetChoice is bigDF:\n",
        "  classNames = [\"other\",\"misinformation\"]\n",
        "elif datasetChoice is littleDF:\n",
        "  classNames = [\"good information\",\"misinformation\"]\n",
        "else:\n",
        "  raise ValueError(\"datasetChoice must be either 'bigDF' or 'littleDF'\")\n",
        "# Creating a Huggingface dataset object from the slimmed-down Pandas dataframe.\n",
        "ds_features = Features({'combined': Value('string'), 'class': ClassLabel(names=classNames)})\n",
        "dataset = Dataset.from_pandas(df=finalDF, features=ds_features,preserve_index=False)"
      ],
      "metadata": {
        "id": "w0MwqIJeKfGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the winning model and tokenizer.\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"smcrone/monkeypox-misinformation\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"smcrone/monkeypox-misinformation\",use_fast=False)"
      ],
      "metadata": {
        "id": "yg-9b-zsFL37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waDl9NiATWft"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the dataset.\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['combined'], padding = \"max_length\", truncation=True)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnMKvbu786f7"
      },
      "outputs": [],
      "source": [
        "# Converting tokenized dataset to Tensorflow format.\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "tf_test_dataset = tokenized_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=['class'],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "mqIDwJDdzpqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **5. Evaluating the model**\n",
        "Call the model on the dataset and evaluate the results.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NZ3HoqnfzdZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model.\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.keras.metrics.SparseCategoricalAccuracy())"
      ],
      "metadata": {
        "id": "80-1sBJfSaOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating classification report based on model predictions.\n",
        "testDataLabels = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
        "modelLogits = model.predict(tf_test_dataset).logits\n",
        "modelPredictions = []\n",
        "for i in range(len(modelLogits)):\n",
        "  prediction = np.argmax(modelLogits[i])\n",
        "  modelPredictions.append(prediction)\n",
        "classificationReport = classification_report(testDataLabels,modelPredictions,digits=5)\n",
        "print(classificationReport)"
      ],
      "metadata": {
        "id": "89Du83201M7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and plotting a confusion matrix based on model predictions.\n",
        "confusionMatrix = confusion_matrix(testDataLabels, modelPredictions, labels=None, sample_weight=None, normalize=None)\n",
        "sns.heatmap(confusionMatrix, square=True, annot=True, cbar=False, cmap=\"Blues\",fmt='g')\n",
        "plt.title(\"Fine-tuned model predictions\")\n",
        "plt.xlabel('predicted value')\n",
        "plt.ylabel('true value')\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "metadata": {
        "id": "_4d28Rlj19vK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPUJzeFs5x8+v5gyhekhVOU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}